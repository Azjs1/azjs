import os
import pandas as pd
import numpy as np
import logging
import joblib
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
MODEL_PATH = "news_model.h5"
TOKENIZER_PATH = "tokenizer.pkl"
MAX_SEQUENCE_LENGTH = 100
VOCAB_SIZE = 5000

def load_news_dataset(filepath="news_dataset.csv"):
    if not os.path.exists(filepath):
        logging.error("âŒ [DATA] Ù…Ù„Ù Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
        return None
    df = pd.read_csv(filepath)
    if "text" not in df.columns or "label" not in df.columns:
        logging.error("âŒ [DATA] ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ù…Ù„Ù Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ÙŠ 'text' Ùˆ 'label'")
        return None
    return df

def train_news_sentiment_model():
    df = load_news_dataset()
    if df is None:
        return

    texts = df["text"].astype(str).tolist()
    labels = pd.get_dummies(df["label"]).values  # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø¥Ù„Ù‰ one-hot

    tokenizer = Tokenizer(num_words=VOCAB_SIZE)
    tokenizer.fit_on_texts(texts)
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

    X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)

    model = Sequential()
    model.add(Embedding(VOCAB_SIZE, 64, input_length=MAX_SEQUENCE_LENGTH))
    model.add(LSTM(64, return_sequences=False))
    model.add(Dropout(0.5))
    model.add(Dense(3, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    logging.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±...")
    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))

    model.save(MODEL_PATH)
    joblib.dump(tokenizer, TOKENIZER_PATH)

    logging.info("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø± Ø¨Ù†Ø¬Ø§Ø­.")

# âœ… Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ù‘Ø¨
def predict_news_sentiment_lstm(text: str) -> float:
    try:
        if not os.path.exists(MODEL_PATH) or not os.path.exists(TOKENIZER_PATH):
            logging.error("âŒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ùˆ Ø§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø± ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
            return 0.0

        tokenizer = joblib.load(TOKENIZER_PATH)
        model = load_model(MODEL_PATH)

        sequence = tokenizer.texts_to_sequences([text])
        padded = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)
        prediction = model.predict(padded)[0]

        # Ù†Ø¹ÙŠØ¯ Ø§Ù„ÙØ§Ø±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ ÙˆØ§Ù„Ø³Ù„Ø¨ÙŠ ÙƒÙ…Ø¤Ø´Ø± Ù…Ø´Ø§Ø¹Ø±
        sentiment_score = prediction[2] - prediction[0]  # pos - neg
        return round(float(sentiment_score), 3)

    except Exception as e:
        logging.error(f"âŒ [PREDICT] ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
        return 0.0

# âœ… ØªØ¯Ø±ÙŠØ¨ Ù…Ø¨Ø§Ø´Ø± Ø¥Ø°Ø§ ØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª
if __name__ == "__main__":
    train_news_sentiment_model()
